# My Junior Year Neural Network Project

What's this repo about and why would I post work that I did in 2007, now in 2024?

This repo contains the material I produced in a final project for a neural network course that I took as an undergraduate.

### But why post this now? 
I was recently asked to explain and implement backpropagation in a machine learning engineer interview. Having not used or studied the algorithm for about 17 years at the time, I was not able to answer the interview question very well. I first learned the backpropagation algorithm in 2007 in a course I took about neural networks in my third year of college as part of my undergraduate curriculum for a degree in mathematical decision science. Some time after the interview I decided to take a walk down memory lane. I pulled out my old Seagate hard-drive that I used to backup work and music in college. The hard-drive booted, I opened the device via my Mac Finder, and sure enough, my project was still there. Very nostolgic. It really brought me back to a time and a mindset when for the most part I was learning just for the sake of learning interesting things. 

Having done so poorly on the interview question, and feeling a little bit inspired to waste time learning something I may not use again for many years, I decided to take some time to review backpropagation. That mini-project is also available in GitHub (https://github.com/SeanSkwerer/backprop). Overall I found that exercise to be very rewarding, and don't often set aside time to learn things that are not necessarily immediately applicable to my day-to-day work.

# What is the project about?

How does the size of a feedforward networks effect its capacity to learn non-linear and topologically varied decision boundaries? I thought of this project because I was taking a course in topology during the same semester. The project used densely sampled simulated data to evaluate if networks of various architectures could learn classification rules where the class regions where geometrically and topologically interesting. Intuitively wider and deeper networks have more capacity to learn more complex functions. This project is an empirical study of the network size required for a few different cases.